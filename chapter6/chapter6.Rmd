# 6章code

```{r}
knitr::opts_chunk$set(warning=FALSE)
```

```{r}
# Package読み込み
library(QSARdata); library(FactoMineR); library(factoextra)
library(caret); library(glmnet); library(xgboost);
library(rBayesianOptimization); library(sessioninfo)
```

```{r}
# データ読みこみ・確認
data(MeltingPoint)
str(MP_Descriptors[, 1:10])
```

```{r}
# データ結合
temp_df <- data.frame(MP_Outcome, MP_Data, MP_Descriptors)

# トレーニング・テスト分割
working_df <- subset(temp_df, MP_Data == "Train")
eval_df <- subset(temp_df, MP_Data == "Test")

# 不要なデータ削除
working_df$MP_Data <- NULL
eval_df$MP_Data <- NULL
```

```{r}
# Package読み込み
library(QSARdata); library(FactoMineR); library(factoextra)
library(caret); library(glmnet); library(xgboost);
library(rBayesianOptimization); library(sessioninfo)
```

```{r}
# データ読みこみ・確認
data(MeltingPoint)
str(MP_Descriptors[, 1:10])
```

```{r}
# データ結合
temp_df <- data.frame(MP_Outcome, MP_Data, MP_Descriptors)

# トレーニング・テスト分割
working_df <- subset(temp_df, MP_Data == "Train")
eval_df <- subset(temp_df, MP_Data == "Test")

# 不要なデータ削除
working_df$MP_Data <- NULL
eval_df$MP_Data <- NULL
```

```{r}
pca_res <- PCA(working_df, # 主成分分析を行うデータの指定
               graph = FALSE, #図 の表示なし
               ncp = 10) 
```

```{r}
fviz_pca_var(pca_res, # 上記で作成・保存したPCAの結果
             axes = c(1, 8), # 表示したい成分の指定
             col.var="contrib", # 寄与率を色で表記
             repel = TRUE,  # ラベルの重なりをなるべく回避
             labelsize = 3,  # ラベルのフォントサイズ
             select.var = list(name = "MP_Outcome") # 表示したい因子
             )
```

```{r}
fviz_contrib(pca_res, # 上記で作成・保存したPCAの結果
             choice = "var", # 変数を指定
             axes = 1,       # 寄与率を見たい成分の指定
             top = 15)        # 上位いくつ目の成分まで表示するか

```

```{r}
fviz_contrib(pca_res, # 上記で作成・保存したPCAの結果
             choice = "var", # 変数を指定
             axes = 8,       # 寄与率を見たい成分の指定
             top = 15)        # 上位いくつ目の成分まで表示するか

```

```{r}
set.seed(71)
tr = trainControl(
  method = "repeatedcv", # 最適化の方法
  number = 5) # 5-fold CV
```

```{r}
train_grid_lasso = expand.grid(alpha = 1 , lambda = 10 ^ (0:10 * -1)) 
```

```{r}
set.seed(71) # 乱数の固定
lasso_fit_reg = train(working_df[, c(2:203)],  # 説明変数
                       working_df$MP_Outcome,   # 目的変数
                       method = "glmnet",   # lassoが含まれるパッケージの指定
                       tuneGrid = train_grid_lasso, # パラメータ探索の設定
                       trControl=tr,                # クロスバリデーションの設定
                       preProc = c("center", "scale"), # 標準化
                       metric = "RMSE")      # 最適化する対象
```

```{r}
lasso_fit_reg
```

```{r}
# トレーニングセットに対する予測値
pred_train_lasso <- predict(lasso_fit_reg, working_df[, c(2:203)]) 

# テストセットに対する予測値
pred_test_lasso <- predict(lasso_fit_reg, eval_df[, c(2:203)])
```

```{r}
# トレーニング・バリデーションデータの読み込み
train_x <- data.matrix(working_df[, c(2:203)])
train_y <- data.matrix(working_df[, 1])

test_x <- data.matrix(eval_df[, c(2:203)])
test_y <- data.matrix(eval_df[, 1])

train <- xgb.DMatrix(train_x, label = train_y)
```

```{r}
# クロスバリデーションの設定
cv_folds <- KFold(train_y, 
                  nfolds = 5,
                  seed = 71)
```

```{r}
# ベイズ最適化の設定
xgb_cv_bayesopt <- function(max_depth, min_child_weight, subsample, lambda, alpha) {
  cv <- xgb.cv(params = list(booster = "gbtree", 
                             eta = 0.1,
                             max_depth = max_depth,
                             min_child_weight = min_child_weight,
                             subsample = subsample, 
                             lambda = lambda, 
                             alpha = alpha,
                             colsample_bytree = 0.7,
                             objective = "reg:linear",
                             eval_metric = "rmse"), # RMSEを設定
               data = train, 
               folds = cv_folds, 
               nround = 1000,
               early_stopping_rounds = 20, 
               maximize = FALSE,  # RMSEなので最小化する
               verbose = 0)
  list(Score = cv$evaluation_log$test_rmse_mean[cv$best_iteration],
       Pred = cv$pred)
}
```

```{r}
# ベイズ最適化の実行・実行に10~30分程度
set.seed(71) 
Opt_res <- BayesianOptimization(xgb_cv_bayesopt,
                                bounds = list(max_depth = c(3L, 7L),
                                              min_child_weight = c(1L, 10L),
                                              subsample = c(0.7, 1.0),
                                              lambda = c(0.5, 1), 
                                              alpha = c(0.0, 0.5)), 
                                init_points = 20, 
                                n_iter = 30,
                                acq = "ucb", 
                                kappa = 5, 
                                verbose = FALSE)
```

```{r}
# ベイズ最適化により最適化されたパラメータの設定
params <- list(
  "booster"             = "gbtree",
  "objective"           = "reg:linear",
  "eval_metric"         = "rmse",
  "eta"                 = 0.1,
  "max_depth"           = 3,
  "min_child_weight"    = 10,
  "subsample"           = 0.7193,
  "colsample_bytree"    = 0.7,
  "alpha"               = 0.5,
  "lambda"              = 0.5
)
```

```{r}
# クロスバリデーションによるearly_stopping_roundsの最適化
set.seed(71)

cv_nround = 1000
cv_test <- xgb.cv(params = params, data = train, nfold = 5, nrounds = cv_nround, 
                 early_stopping_rounds = 20, maximize = FALSE, verbose = FALSE)

cv_nround <- cv_test$best_iteration
```

```{r}
# 最適化したパラメータを用いて予測モデル構築・予測値の算出
set.seed(71)
model <- xgboost(data = train, 
                 params = params, 
                 nrounds = cv_nround, 
                 verbose = FALSE)

pred_train <- predict(model, train_x)
pred_test <- predict(model, test_x)
```

```{r}
plot(pred_test_lasso,     # テストセットの予測値
     eval_df$MP_Outcome,  # テストセットの実測値
     pch = 16,            # プロットのマーク
     col = 6,             # プロットの色
     xlim = c(0, 400), 
     ylim = c(0, 400), 
     ann = F)             # 軸、タイトルを非表示

par(new = T)   # 次の入力を重ね書きする

plot(pred_train_lasso,       # トレーニングセットの予測値
     working_df$MP_Outcome,  # トレーニングセットの実測値
     pch = 21,               # プロットのマーク
     col = 1,                # プロットの色
     xlim = c(0, 400), 
     ylim = c(0, 400))
```

```{r}
plot(pred_test,           # テストセットの予測値
     eval_df$MP_Outcome,  # テストセットの実測値
     pch = 16,            # プロットのマーク
     col = 6,             # プロットの色
     xlim = c(0, 400), 
     ylim = c(0, 400), 
     ann = F)             # 軸、タイトルを非表示

par(new = T)   # 次の入力を重ね書きする

plot(pred_train,             # トレーニングセットの予測値
     working_df$MP_Outcome,  # トレーニングセットの実測値
     pch = 21,               # プロットのマーク
     col = 1,                # プロットの色
     xlim = c(0, 400), 
     ylim = c(0, 400))
```

```{r}
plot(varImp(lasso_fit_reg), top = 20)
```

```{r}
importance <- xgb.importance(colnames(test_x), model = model)
xgb.ggplot.importance(importance, top_n = 20)
```

```{r}
options(width = 100) # session_info()出力の幅が広いため調整
```

```{r}
session_info()
```
